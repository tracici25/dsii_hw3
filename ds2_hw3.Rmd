---
title: "HW3"
output: html_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(ISLR)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
```


# Part A. Graphic Summaries
```{r}
data("Weekly")
summary(Weekly)
transparentTheme(trans = .4)
featurePlot(x = Weekly[, 1:8], 
            y = Weekly$Direction,
            scales = list(x=list(relation="free"), 
                        y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2)) # density plot
pairs(Weekly) # pairs scatterplot
```


# Part B. 

```{r}
train = Weekly %>% 
  filter(Year %in% c(1990:2008))
test = Weekly %>% 
  filter(Year %in% c(2009,2010))
glm_fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
               data = train,
               family = binomial)
summary(glm_fit)
```

From performing logistic regression on the training data, it is found that 'Lag1' is a significant predictor due to its p-value < 0.05.

```{r}
# Confusion Matrix
prob = predict(glm_fit, newdata = test, type = "response")
pred = rep("Down", length(prob))
pred[prob > 0.5] = "Up"
confusionMatrix(data = as.factor(pred), reference = test$Direction, positive = "Up")
```

* From the confusion matrix, the overall fraction of correct predictions using the testing data is 0.4615, 95% CI (0.3633, 0.562). 

* P-value = 0.9962 > 0.05, indicating we fail to reject the null hypothesis and accuracy = NIR(NIR = 0.5865). (max((True Positive + False Positive)/n, (False Negative + True Negative)/n))  

* Kappa = -3*10^-4, indicating the model doesn't fit the data well. Kappa closer to 1, better the model fits.

* Sensitivity = 0.2787 [True Positive/(True Positive + False Negative)], 27.87% of true positives are correctly identified.

* Specificity = 0.7209 [True Negative/(True Negative + False Positive)], 72.09% of true negatives are correctly identified.







