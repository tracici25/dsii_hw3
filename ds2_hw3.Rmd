---
title: "HW3"
output: html_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(ISLR)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
```


# Part A. Graphic Summaries
```{r}
data("Weekly")
summary(Weekly)
transparentTheme(trans = .4)
featurePlot(x = Weekly[, 1:8], 
            y = Weekly$Direction,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2)) # density plot
pairs(Weekly) # pairs scatterplot
```


# Part B. 

```{r}
train = Weekly %>% 
  filter(Year <= 2008)
test = Weekly %>% 
  filter(Year > 2008)
glm_fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
              data = train,
              family = binomial)
summary(glm_fit)
```

From performing logistic regression on the training data, it is found that 'Lag1' is a significant predictor due to its p-value < 0.05.

```{r}
# Confusion Matrix
prob = predict(glm_fit, newdata = test, type = "response")
pred = rep("Down", length(prob))
pred[prob > 0.5] = "Up"
confusionMatrix(data = as.factor(pred), reference = test$Direction, positive = "Up")
```

* From the confusion matrix, the overall fraction of correct predictions using the testing data is 0.4615, 95% CI (0.3633, 0.562). 

* P-value = 0.9962 > 0.05, indicating we fail to reject the null hypothesis and accuracy = NIR(NIR = 0.5865). (max((True Positive + False Positive)/n, (False Negative + True Negative)/n))  

* Kappa = -3*10^-4, indicating the model doesn't fit the data well. Kappa closer to 1, better the model fits.

* Sensitivity = 0.2787 [True Positive/(True Positive + False Negative)], 27.87% of true positives are correctly identified.

* Specificity = 0.7209 [True Negative/(True Negative + False Positive)], 72.09% of true negatives are correctly identified.

# Part C. 

```{r}
glm_fit2 = glm(Direction ~ Lag1 + Lag2, data = train, family = binomial)
summary(glm_fit2)

###Plot the ROC curve using the test data and report the AUC
glm2_prob = predict(glm_fit2, newdata = test, type = "response")
roc = roc(test$Direction, glm2_prob)
plot(roc, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc), col = 4, add = TRUE)
```

From the ROC curve, AUC = 0.556.

# Part D. 

## LDA
```{r}
lda_fit = lda(Direction ~ Lag1 + Lag2, data = train)
lda_pred = predict(lda_fit, newdata = test)
roc_lda = roc(test$Direction, lda_pred$posterior[,2], levels = c("Down", "Up"))
plot(roc_lda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_lda), col = 4, add = TRUE)
```

LDA AUC = 0.557.

## QDA
```{r}
qda_fit = qda(Direction ~ Lag1 + Lag2, data = train)
qda_pred = predict(qda_fit, newdata = test)
roc_qda = roc(test$Direction, qda_pred$posterior[,2], levels = c("Down", "Up"))
plot(roc_qda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_qda), col = 4, add = TRUE)
###AUC is 0.529
```

QDA AUC = 0.529.

# Part E.

```{r}
set.seed(1)
ctrl = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
fit_knn <- train(x = train[2:3],
                 y = train$Direction,
                 method = "knn",
                 preProcess = c("center", "scale"),
                 tuneGrid = data.frame(k = seq(1, 200, by = 5)),
                 trControl = ctrl)
summary(fit_knn)
ggplot(fit_knn)
fit_knn$bestTune
knn_pred =  predict(fit_knn, newdata = test , type = "prob")
roc_knn = roc(test$Direction, knn_pred$Down, levels = c("Down", "Up"))
plot(roc_knn, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_knn), col = 4, add = TRUE)
```

The KNN AUC = 0.437. Comparing with GLM, LDA, QDA and KNN, LDA has the largest AUC, indicating LDA predicts the data best. Due to its high error, further investigation on better models is required.

